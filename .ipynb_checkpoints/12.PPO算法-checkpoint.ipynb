{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1777220b-3079-46c2-9cc9-46180da0c42d",
   "metadata": {},
   "source": [
    "# 12. PPO 算法\n",
    "\n",
    "## 12.1 简介\n",
    "第 11 章中介绍的 TRPO 算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。于是， TRPO 算法的改进版—— PPO 算法在 2017 年被提出， PPO 基于 TRPO 的思想，但是其算法实现更加简单。并且大量的实验结果表明，与 TRPO 相比，PPO 能学习得一样好（甚至更快），这使得 PPO 成为非常流行的强化学习算法，如果我们想要尝试在一个新的环境中使用强化学习算法，那么 PPO 就属于可以首先尝试的算法。\n",
    "\n",
    "回忆一下 TRPO 的优化目标：\n",
    "$$\n",
    "\\max_{\\theta}\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_{k}}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_{k}}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}A^{\\pi_{\\theta_{k}}} (s,a)\\right]\n",
    "$$\n",
    "$$\n",
    "\\mathrm{s.t.}\\quad\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_{k}}}}[D_{KL}(\\pi_{\\theta_{k}}(\\cdot|s),\\pi_{\\theta}(\\cdot|s))]\\leq\\delta\n",
    "$$\n",
    "\n",
    "TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO 的优化目标与 TRPO 相同，但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断，我们接下来对这两种形式进行介绍\n",
    "\n",
    "## 12.2 PPO-惩罚\n",
    "PPO-惩罚（PPO-Penalty）用拉格朗日乘数法直接将 KL 散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，在迭代的过程中不断更行 KL 散度前的系数，即\n",
    "$$\n",
    "\\arg\\max_{\\theta}\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_{k}}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_{k}}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}A^{\\pi_{\\theta_{k}}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_{k}}(\\cdot|s),\\pi_{\\theta}(\\cdot|s)]\\right]\n",
    "$$\n",
    "令 $d_k=D_{KL}^{\\nu^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_\\theta)$，$\\beta$ 的更新规则如下：\n",
    "1. 如果 $d_k < \\delta/1.5$，那么 $\\beta_{k+1} = \\beta_k / 2$\n",
    "2. 如果 $d_k > \\delta \\times 1.5$，那么 $\\beta_{k+1} = \\beta_k \\times 2$\n",
    "3. 否则 $\\beta_{k+1} = \\beta_k$\n",
    "\n",
    "其中，$\\delta$ 是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距\n",
    "\n",
    "## 12.3 PPO-截断\n",
    "PPO 的另一种形式 PPO-截断（PPO-Clip）更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，即\n",
    "$$\n",
    "\\arg\\max_{\\theta}\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_{k}}(\\cdot|s)}\\left[\\min\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}A^{\\pi_{\\theta_{k}}}(s,a),\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)},1-\\epsilon,1+\\epsilon\\right)A^{\\pi_{\\theta_{k}}}(s,a)\\right)\\right]\n",
    "$$\n",
    "其中 $\\text{clip}(x,l,r):=\\max(\\min(x,r),l)$，即把 $x$ 限制在 $[l,r]$ 内，上式中 $\\epsilon$ 是一个超参数，表示进行截断（clip）的范围\n",
    "如果 $A^{\\pi_{\\theta_k}}(s,a) > 0$，说明这个动作的价值高于平均，最大化这个式子会增大 $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}$，但不会让其超过 $1 + \\epsilon$。反之，如果 $A^{\\pi_{\\theta_k}}(s,a) < 0$，最大化这个式子会减小 $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}$，但不会让其超过 $1 - \\epsilon$。如图 12-1 所示。\n",
    "<div align=\"center\">\n",
    "    <img src=\"./image/12-1.png\">\n",
    "    <center>图12-1 PPO-截断示意图</center>\n",
    "</div> \n",
    "\n",
    "## 12.4 PPO 代码实践\n",
    "与 TRPO 相同，我们仍然在车杆和倒立摆两个环境中测试 PPO 算法。大量实验表明，PPO-截断总是比 PPO-惩罚表现得更好。因此下面我们专注于 PPO-截断的代码实现。\n",
    "首先导入一些必要的库，并定义策略网络和价值网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fad379-59b7-4e8b-95ef-e5c67c2043c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
